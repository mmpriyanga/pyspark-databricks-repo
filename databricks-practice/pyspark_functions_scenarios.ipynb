{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb4b5e0-39a9-42a4-8bfc-a9654486b6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "schema = StructType([StructField('id', IntegerType(),True),\n",
    "                     StructField('name',StringType(),True),\n",
    "                     StructField('Department',StringType(),True),\n",
    "                     StructField('Salary',IntegerType(),True),\n",
    "                     ])\n",
    "data = [(1,'Mukil','IT',2000),(2,'Priyanga','IT',3000),(3,'Mani','HR',1000),(4,'Lekha','Finance',1400),(5,'Priya','Finance',3000)]\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff10e71-fe34-48ba-936e-f4c03ef52786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "from pyspark.sql import functions as F\n",
    "df.groupBy('Department').agg({'Salary':'avg'}).alias('Avg_Salary').show() #Alias not working\n",
    "df.groupBy('Department').agg(F.sum('Salary').alias('Total_Salary')).show()\n",
    "'''\n",
    "The number of employees\n",
    "The employee with the highest salary in that department\n",
    "'''\n",
    "df.groupBy('Department').agg(F.count('id').alias('Emp_Count')).show()\n",
    "df.groupBy('Department').agg(F.max('Salary').alias('Max_Salary')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3868aaac-aa3f-4d9c-9e50-d7955ad489fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Windowing\n",
    "#Find the highest Salary per department\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "window_spec = Window.partitionBy('Department').orderBy(F.desc('Salary'))\n",
    "df_ranked = df.withColumn('rank', F.rank().over(window_spec))\n",
    "df_ranked.filter(col('rank') == 1).select(col('name'),col('Salary'),col('Department')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511b70b4-3888-4b48-9df2-a376f245c8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"bonus\", DoubleType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\",   \"IT\",       4000, 500.0,  \"New York\"),\n",
    "    (2, \"Bob\",     \"IT\",       4500, 600.0,  \"Chicago\"),\n",
    "    (3, \"Charlie\", \"HR\",       3000, 400.0,  \"New York\"),\n",
    "    (4, \"David\",   \"Finance\",  3500, 200.0,  \"Boston\"),\n",
    "    (5, \"Eve\",     \"Finance\",  3800, 250.0,  \"Chicago\"),\n",
    "    (6, \"Frank\",   \"HR\",       3200, 300.0,  \"Boston\"),\n",
    "    (7, \"Grace\",   \"IT\",       5000, 700.0,  \"New York\"),\n",
    "    (8, \"Helen\",   \"Finance\",  4200, 350.0,  \"Chicago\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ceba62-3aa2-4a72-838d-462c28cf8206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Find the average salary and total bonus per department.\n",
    "Find the top 2 highest-paid employees in each department using a window function.\n",
    "Find the total salary per city and department combination.\n",
    "Find employees who earn more than the average salary of their department\n",
    "Add a new column showing each employee’s salary rank within their department.\n",
    "Calculate total compensation (salary + bonus) and find the top 3 earners overall.\n",
    "Pivot the DataFrame to show total salary for each department across cities.\n",
    "Get the department that has the highest average bonus.\n",
    "Find the difference between each employee’s salary and the maximum salary in their department.\n",
    "Join this DataFrame with another (say, department head info) and find which head manages the highest total salary.\n",
    "'''\n",
    "\n",
    "df.groupBy('Department').agg(F.round(F.avg('salary'),2).alias('avg_sal'),F.sum('bonus').alias('sum_bonus')).show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "window_spec = Window.partitionBy('Department').orderBy(F.desc('salary'))\n",
    "df_ranked = df.withColumn('ranked', F.dense_rank().over(window_spec))\n",
    "df_ranked.filter(col('ranked')<=2).select(col('name'), col('Department'),col('salary')).show()\n",
    "\n",
    "df.groupBy('city','Department').agg(F.sum('salary').alias('sum_city_dept')).show()\n",
    "\n",
    "window_spec = Window.partitionBy('Department')\n",
    "df_avg = df.withColumn('Avg_Salary', F.avg('salary').over(window_spec))\n",
    "df_avg.filter(col('salary') > col('Avg_Salary')).select(col('name'),col('salary'),col('Avg_Salary')).show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "window_spec = Window.partitionBy('Department').orderBy(F.desc('salary'))\n",
    "df.withColumn('ranked', F.dense_rank().over(window_spec)).show()\n",
    "\n",
    "df.withColumn('bonus_salary', col('bonus')+col('salary')).orderBy(col('bonus_salary').desc()).show(3)\n",
    "\n",
    "df.groupBy('Department').pivot('city').agg(F.sum('salary')).show()\n",
    "\n",
    "df.groupBy('Department').agg(F.avg('bonus')).show(1)\n",
    "\n",
    "win_spec = Window.partitionBy('Department')\n",
    "df_max = df.withColumn('max_Salary', F.max('salary').over(win_spec))\n",
    "df_max.withColumn('difference', col('max_Salary')-col('salary')).show()\n",
    "\n",
    "df_joined = df.join(df_head, on='Department', how='inner')\n",
    "df_joined = df_joined.groupBy('head_name').agg(F.sum('salary').alias('tot_head_sal'))\n",
    "df_joined.show()\n",
    "df_joined = df_joined.orderBy(col('tot_head_sal').desc()).show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b45926-c94e-4c29-b0da-44c7ceba56d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "head_schema = StructType([\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"head_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "head_data = [\n",
    "    (\"IT\", \"Mr. John\"),\n",
    "    (\"HR\", \"Ms. Laura\"),\n",
    "    (\"Finance\", \"Mr. Steve\")\n",
    "]\n",
    "\n",
    "df_head = spark.createDataFrame(head_data, head_schema)\n",
    "df_head.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4e5b19-f883-413b-b9d4-a25de9fd3394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#For each employee, calculate cumulative salary and cumulative bonus within their department, ordered by salary descending.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "window_spec = Window.partitionBy('Department').orderBy(col('salary').desc())\n",
    "df.withColumn('Cum_Sal', F.sum('salary').over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f4ec1e-3382-4df1-b42e-757f425e7c14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rank employees first by department, then within city, based on total compensation (salary + bonus).\n",
    "df_tot_comp = df.withColumn('total_compensation', col('salary')+col('bonus'))\n",
    "winspec = Window.partitionBy('Department', 'city').orderBy(col('total_compensation').desc())\n",
    "df_tot_comp.withColumn('ranked', F.dense_rank().over(winspec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b56f6c1-a1a5-4013-b640-cd540e623310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "7#For each department, compute the salary difference with the previous highest-paid employee.\n",
    "\n",
    "win_spec = Window.partitionBy('Department').orderBy(col('salary').desc())\n",
    "df_lead = df.withColumn('lagSal', F.lag('salary').over(win_spec))\n",
    "df_lead.withColumn('sal_diff', col('lagSal')-col('salary')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa708b1-2fbd-4e7d-9d42-318ff8ffa8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "  {\"id\": 1, \"name\": \"Alice\", \"city\": \"New York\"},\n",
    "  {\"id\": 2, \"name\": \"Bob\", \"city\": \"Los Angeles\"}\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab29252-3226-445a-8352-8e035ebb7799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"address\": {\n",
    "      \"city\": \"New York\",\n",
    "      \"state\": \"NY\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"address\": {\n",
    "      \"city\": \"Los Angeles\",\n",
    "      \"state\": \"CA\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.select(\n",
    "    'id',\n",
    "    'name',\n",
    "    col('address.city').alias('address_city'),\n",
    "    col('address.state').alias('address_state')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa769ac7-eb49-4f52-b641-7215bbcffc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"phoneNumbers\": [\n",
    "      {\"type\": \"home\", \"number\": \"555-1234\"},\n",
    "      {\"type\": \"mobile\", \"number\": \"555-5678\"}\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"phoneNumbers\": [\n",
    "      {\"type\": \"home\", \"number\": \"555-8765\"}\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "from pyspark.sql.functions import explode\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)\n",
    "df_new = df.select(\n",
    "    'id',\n",
    "    'name',\n",
    "    explode('phoneNumbers').alias('exploded_phoneNumber')\n",
    ")\n",
    "df_new = df_new.select(\n",
    "    'id',\n",
    "    'name',\n",
    "    col('exploded_phoneNumber.type').alias('phonenum_type'),\n",
    "    col('exploded_phoneNumber.number').alias('phonenum_number')\n",
    ")\n",
    "df_new.show()\n",
    "df_new.write.format('csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb179d8e-ce33-4f16-b5cc-93e68d840c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write logic to classify employees:\n",
    "“High” → total_comp > 100000\n",
    "“Medium” → 50000 ≤ total_comp ≤ 100000\n",
    "“Low” → below 50000\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col, when\n",
    "df_tot = df.withColumn('total_comp', col('salary')+col('bonus'))\n",
    "df_tot.withColumn('emp_class', when(col('total_comp')> 5000, 'High').\n",
    "                               when((col('total_comp')<= 5000) & (col('total_comp')>= 4000), 'Medium').otherwise('Low')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e990d481-594a-47a6-a231-2fb0c739ee8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ----------------------\n",
    "# Create DataFrame\n",
    "# ----------------------\n",
    "data = [\n",
    "    (\n",
    "        1,\n",
    "        100,\n",
    "        '[{\"sku\":\"A1\",\"qty\":2,\"price\":10.0},{\"sku\":\"B2\",\"qty\":1,\"price\":5.5}]',\n",
    "        \"2025-10-01\"\n",
    "    ),\n",
    "    (\n",
    "        2,\n",
    "        101,\n",
    "        '[{\"sku\":\"A1\",\"qty\":1,\"price\":10.0}]',\n",
    "        \"2025-10-02\"\n",
    "    )\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"order_id\", \"customer_id\", \"items_json\", \"order_date\"])\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75730aba-5c22-44f1-902d-b38358f5fc60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "item_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField('sku', StringType()),\n",
    "        StructField('qty', IntegerType()),\n",
    "        StructField('price', DoubleType())\n",
    "    ])\n",
    ")\n",
    "\n",
    "df_parsed = df.withColumn('items', from_json(col('items_json'), item_schema))\n",
    "df_parsed = df_parsed.withColumn(\"item\", explode('items'))\n",
    "df_parsed = df_parsed.withColumn('sku', col(\"item.sku\")).withColumn('quantity', col('item.qty')).withColumn('price', col('item.price'))\n",
    "df_selected = df_parsed.select('order_id', 'customer_id','order_date', 'sku', 'quantity', 'price')\n",
    "df_selected.printSchema()\n",
    "df_updated = df_selected.withColumn('total', col('quantity').cast('long') * col('price').cast('double'))\n",
    "df_grouped = df_updated.groupBy('order_id').agg(sum('total').alias('order_total'))\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4035e3-b4cd-4431-8b36-80ad49683fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"2025-11-01 10:00:00\", \"OPEN\", 1, \"systemA\"),\n",
    "    (1, \"2025-11-01 11:00:00\", \"CLOSED\", 2, \"systemB\"),\n",
    "    (1, \"2025-11-01 11:00:00\", \"OPEN\", 1, \"systemC\"),\n",
    "    (2, \"2025-11-02 09:00:00\", \"OPEN\", 1, \"systemA\"),\n",
    "    (2, \"2025-11-02 09:05:00\", \"OPEN\", 2, \"systemA\"),\n",
    "    (2, \"2025-11-02 09:05:00\", \"CLOSED\", 3, \"systemB\"),\n",
    "    (3, \"2025-11-05 12:20:00\", \"PENDING\", 1, \"systemA\"),\n",
    "    (3, \"2025-11-05 12:20:00\", \"PENDING\", 2, \"systemB\"),\n",
    "    (3, \"2025-11-05 12:30:00\", \"OPEN\", 1, \"systemA\"),\n",
    "    (4, \"2025-11-10 14:00:00\", \"RESOLVED\", 5, \"systemA\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"event_time\", \"status\", \"priority\", \"source\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fd03f6-84a9-45a8-ad61-67cd766e79ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#latest record per ID\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy('id').orderBy(col('event_time').desc(), col('priority').desc())\n",
    "df_win = df.withColumn('rank', dense_rank().over(window_spec)).filter(col('rank') == 1)\n",
    "df_win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0084a1cf-b46c-4ec7-98a9-c950d8c6ab3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag,col, unix_timestamp\n",
    "win_spec = Window.partitionBy('id').orderBy(col('event_time'))\n",
    "df_updated  = df.withColumn('lag_col', lag('event_time').over(win_spec))\n",
    "df_upd_timediff = df_updated.withColumn('time_diff_min', (unix_timestamp('event_time')-unix_timestamp('lag_col'))/60)\n",
    "df_prev_status = df_upd_timediff.withColumn('prev_status', lag('status').over(win_spec))\n",
    "df_prev_status = df_prev_status.filter(col('prev_status').isNotNull()).select('id', 'prev_status', col('status').alias('curr_status'), 'time_diff_min')\n",
    "df_prev_status.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7bb5837-d36e-4330-a0a5-f5573ddcee34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8f827e-6177-45f1-8d18-aa08aaa35f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df_prev_status = df.withColumn('prev_status', lag('status').over(win_spec)).filter(col('prev_status').isNotNull())\n",
    "df_count = df_prev_status.groupBy('id').agg(countDistinct('prev_status').alias('status_change_count'))\n",
    "df_result = df.select(\"id\").distinct() \\\n",
    "    .join(df_count, \"id\", \"left\") \\\n",
    "    .fillna(0, subset=[\"status_change_count\"])\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1780423-eb3e-4394-b97c-a2e027bafa21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_int = df.withColumn('date_col', F.to_date('event_time'))\n",
    "df_int.groupBy('date_col').agg(col('priority').max()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "207a2867-fda3-4df2-9898-028cd91155cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_functions_scenarios",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
